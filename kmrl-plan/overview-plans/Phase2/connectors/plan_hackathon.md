Plan: 
Gmail/Drive â†’ historical sync â†’ incremental sync â†’ pull all files â†’ scan â†’ reject/accept â†’ store raw in MinIO + metadata in Postgres â†’ push ID to Redis/Celery queue for extraction.

Metadata fields: id, minio_id, source_system, source_id, file_name, file_type, mime_type, file_size, ingestion_ts, department, doc_category, checksum, status, rejection_reason (+optional language, sender, tags).

DAGS
gmail_connector -> scan_and_store -> push_to_queue
drive_connector -> scan_and_store -> push_to_queue

Where:
gmail_connector / drive_connector â†’ fetch files.
scan_and_store â†’ virus scan + filter â†’ store in MinIO + metadata in Postgres.
push_to_queue â†’ enqueue doc_id for extraction.

Universal Log
Every connector should write to a **universal log table** (`ingestion_log`) or a central logging system:
* connector name
* action (fetch, scan, reject, store)
* status (success/failure)
* reason (if fail)
* timestamp
This gives **traceability** and helps debugging.


## ðŸ”¹ Hackathon Workflow (step-by-step)

1. **Connector (Gmail / Drive)**

   * Run once for **historical sync** (pull 50â€“100 docs).
   * Switch to **incremental sync** (poll new docs).

2. **File Handling**

   * Pull everything â†’ scan with ClamAV â†’ reject unsupported file types (keep a log entry).

3. **Storage**

   * **Raw file** â†’ store in MinIO bucket (per connector, e.g., `gmail/2025/10/file.pdf`).
   * **Metadata** â†’ store record in Postgres.

4. **Queue**

   * Push a message (doc_id, minio_id, metadata_id) into Redis/Celery queue.
   * Workers consume â†’ do extraction later.

---

## ðŸ”¹ 4. Metadata Schema (minimum needed)

Hereâ€™s a solid schema for Postgres `documents` table:

| Field                | Type               | Purpose                                                    |
| -------------------- | ------------------ | ---------------------------------------------------------- |
| **id** (PK)          | UUID / serial      | Unique identifier for pipeline                             |
| **minio_id**         | VARCHAR            | Path/ID of file in MinIO                                   |
| **source_system**    | VARCHAR            | Gmail, Drive, etc.                                         |
| **source_id**        | VARCHAR            | Gmail msg_id, Drive file_id (for dedupe)                   |
| **file_name**        | VARCHAR            | Original filename                                          |
| **file_type**        | VARCHAR            | PDF, DOCX, XLSX, PNG                                       |
| **mime_type**        | VARCHAR            | MIME info (application/pdf etc.)                           |
| **file_size**        | INT                | Size in bytes                                              |
| **ingestion_ts**     | TIMESTAMP          | When it was ingested                                       |
| **department**       | VARCHAR (nullable) | If you can derive (HR, Finance, Engineering)               |
| **doc_category**     | VARCHAR (nullable) | Derived from filename (e.g., â€œinvoiceâ€, â€œsafety_circularâ€) |
| **checksum**         | VARCHAR            | SHA256 hash (for dedupe)                                   |
| **status**           | VARCHAR            | success / quarantined / rejected                           |
| **rejection_reason** | VARCHAR (nullable) | why it was dropped (unsupported type, malware)             |

ðŸ‘‰ Later you can add:

* **language** (English, Malayalam, bilingual â€” from langdetect).
* **tags** (if keyword extraction is run early).
* **uploader / sender** (from email headers or Drive metadata).

---

## ðŸ”¹ 5. Universal Log

Every connector should write to a **universal log table** (`ingestion_log`) or a central logging system:

* connector name
* action (fetch, scan, reject, store)
* status (success/failure)
* reason (if fail)
* timestamp

This gives **traceability** and helps debugging.

---

## ðŸ”¹ 6. Hackathon DAG (connectors only)

```python
gmail_connector -> scan_and_store -> push_to_queue
drive_connector -> scan_and_store -> push_to_queue
```

Where:

* `gmail_connector` / `drive_connector` â†’ fetch files.
* `scan_and_store` â†’ virus scan + filter â†’ store in MinIO + metadata in Postgres.
* `push_to_queue` â†’ enqueue doc_id for extraction.

---

âœ… **Answer Summary**

* **Queue**: not strictly needed to *fetch* files, but needed **after storage** to decouple processing.
* **Modular microservices**: each connector is an independent, reusable script/service following the same pattern.
* **Hackathon plan**: Gmail/Drive â†’ historical sync â†’ incremental sync â†’ pull all files â†’ scan â†’ reject/accept â†’ store raw in MinIO + metadata in Postgres â†’ push ID to Redis/Celery queue for extraction.
* **Metadata fields**: `id, minio_id, source_system, source_id, file_name, file_type, mime_type, file_size, ingestion_ts, department, doc_category, checksum, status, rejection_reason (+optional language, sender, tags).`


